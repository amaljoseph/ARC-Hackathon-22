{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7dbbcbf-9b84-475e-a6aa-5d501bc5fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_df = pd.read_csv('tweets.csv', delimiter=';', dtype='unicode', usecols=['user', 'timestamp', 'replies', 'likes', 'retweets', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2008c252-afcd-4c50-a997-324f42cc505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user', 'timestamp', 'replies', 'likes', 'retweets', 'text'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_df = tweets_df[['user', 'timestamp', 'replies', 'likes', 'retweets', 'text']]\n",
    "tweets_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa09c0a-eba1-4633-a7cc-f06f26cce3d6",
   "metadata": {},
   "source": [
    "## Preprocessing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a12e71-ffbd-4724-8c4a-bd1fb0c297a4",
   "metadata": {},
   "source": [
    "* Understand timestamps, remove/extract url info, select english tweets, remove @, # etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59a29081-599b-4987-8e89-be8f281f338c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20165013, 6)\n",
      "(15638471, 6)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_df.shape)\n",
    "# Removing datapoints where tweets are not present\n",
    "tweets_df = tweets_df.dropna(subset=['text'])\n",
    "\n",
    "# Removing duplicate tweets by same user\n",
    "tweets_df = tweets_df.drop_duplicates(subset=['text', 'user'])\n",
    "\n",
    "# Removing UTC offest from timestamp\n",
    "tweets_df['timestamp'] = pd.to_datetime(tweets_df.timestamp).dt.tz_convert(None)\n",
    "print(tweets_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2aaca9-54be-4a93-b470-117821bd2033",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stratifying the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e7942b3-30e5-405a-a5eb-09f440726d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = tweets_df.sample(n=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc79b6ee-f10e-4cd8-80cb-6b4563222c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning/Stratifying the timestamps into hours or days.\n",
    "tweets_df['day_interval'] = tweets_df.timestamp.dt.floor('D')\n",
    "tweets_df['hour_interval'] = tweets_df.timestamp.dt.floor('H')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d4b66-71a1-4b59-b7a2-5ae30e5bb5a9",
   "metadata": {},
   "source": [
    "#### FIlter Daterange between 2012-01-01 00:00:00 and 2019-10-31 23:59:59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce329b1c-d3b0-4b67-bb1a-19963aa13f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = tweets_df[((tweets_df.timestamp >= '2016-01-01 00:00:00') & (tweets_df.timestamp <= '2018-12-31 23:59:59'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb71c56-a7be-470c-b9d3-b7128a6542bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove tweets that have less than 5 words\n",
    "tweets_df =tweets_df[tweets_df.text.str.split().apply(len)>=5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa698413-5071-4604-b48f-10e5e570dc4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select ENglish Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b576d3d2-ceb3-43f2-9d78-70522835e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English tweets\n",
    "import langdetect\n",
    "langdetect.DetectorFactory.seed = 0\n",
    "def english_detector(text):\n",
    "    try:\n",
    "        return langdetect.detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "tweets_df = tweets_df[tweets_df.text.apply(english_detector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbf32eb3-9d1a-4d3c-b4be-e852fc007bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def extract_url_title(url):\n",
    "    try:\n",
    "        reqs = requests.get(url)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        return soup.title.get_text()\n",
    "    except:\n",
    "        return None\n",
    "def remove_url(tweet):\n",
    "    url_regex = '(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'\n",
    "    return re.sub(url_regex, '', tweet)\n",
    "tweets_df['text'] = tweets_df.text.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c97c733-4e6a-4dac-9259-690e5366e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/josepham/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('wordnet')\n",
    "word_set = set(words.words())\n",
    "count_u, count_h = 0, 0\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = remove_url(tweet)\n",
    "    global word_set\n",
    "    word_list = tweet.split(' ')\n",
    "    for id, word in enumerate(word_list):\n",
    "        if word.startswith('@'):\n",
    "            word_list[id] = 'USER' # Replacing hashtags with a keyword USER\n",
    "        if word.startswith('#'):\n",
    "            word_list[id] = word[1:] if word in word_set else '' # Removing hashtag text if its not a known english word. Also removing the # symbol\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "    \n",
    "tweets_df['text'] = tweets_df.text.apply(preprocess_tweet)\n",
    "tweets_df = tweets_df.dropna(subset=['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fedee7-9f58-4bc2-b1c7-fade5e84cfc8",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "791d9e97-2be1-4bd8-af5b-b941f7be15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def apply_vader(tweet): \n",
    "    polarity_scores = analyzer.polarity_scores(tweet)\n",
    "    return pd.Series([polarity_scores['pos'], polarity_scores['neg'], polarity_scores['compound']])\n",
    "\n",
    "tweets_df[['positive_polarity', 'negative_polarity', 'compound']] = tweets_df.text.apply(apply_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "814ce394-64b9-45e5-8eaf-5ca9bf8f465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_csv('tweets_dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24168c36-fe1e-43ef-aa93-4adc197f6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbcf90-a4b1-4916-a714-efa5ab5c942e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon22",
   "language": "python",
   "name": "hackathon22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
